<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Journey Through Neural Network History</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            font-size: 150%;
        }
        section {
            margin-bottom: 20px;
            padding: 20px;
            background-color: #ffffff;
            display: none;
            opacity: 0;
            transition: opacity 0.5s ease-in;
        }
        h1, h2, h3, h4 {
            color: #333;
            margin-top: 20px;
        }
        p, li {
            line-height: 1.6;
            color: #444;
            margin-bottom: 20px;
        }
        ul {
            padding-left: 20px;
        }
        .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            text-align: left;
        }
        .image-placeholder img, .interactive-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .vocab-section {
            background-color: #f0f8ff;
        }
        .vocab-section h3 {
            color: #1e90ff;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .vocab-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .vocab-term {
            font-weight: bold;
            color: #1e90ff;
        }
        .why-it-matters {
            background-color: #ffe6f0;
        }
        .why-it-matters h3 {
            color: #d81b60;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .stop-and-think {
            background-color: #e6e6ff;
        }
        .stop-and-think h3 {
            color: #4b0082;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .continue-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #007bff;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .reveal-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #4b0082;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .test-your-knowledge {
            background-color: #e6ffe6;
        }
        .test-your-knowledge h3 {
            color: #28a745;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .test-your-knowledge h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .test-your-knowledge p {
            margin-bottom: 15px;
        }
        .check-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #28a745;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
            border: none;
            font-size: 1em;
        }
        .faq-section {
            background-color: #fffbea;
        }
        .faq-section h3 {
            color: #ffcc00;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .faq-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <section id="section1">
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="An abstract image showcasing a network of interconnected nodes, symbolizing the concept of neural networks and their complex structure.">
        </div>
        <h1>A Journey Through Neural Network History</h1>
        
        <p>Welcome to the exciting world of neural networks! Ever wondered how machines can learn to 'see' or 'think'? We're going to embark on a journey through time, starting way back in 1943 when the very first idea of an artificial neuron was born. Buckle up, because we are going on a ride!</p>
        <div class="continue-button" onclick="showNextSection(2)">Continue</div>
    </section>

    <section id="section2">
        <p>Our story begins in <strong>1943</strong>. Two brilliant minds, <strong>Warren McCulloch</strong> and <strong>Walter Pitts</strong>, proposed the first mathematical model of a biological neuron. They called it the <strong>Threshold Logic Unit (TLU)</strong>. It was a simple model, but it laid the groundwork for everything that followed. It was able to do simple logic operations, like AND, OR and NOT.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A simplified diagram of the Threshold Logic Unit (TLU). The diagram shows inputs, which are either 0 or 1, going into a circle that represents the TLU. Inside the circle is a summation symbol and a threshold symbol. The output, either 0 or 1, is shown coming out of the TLU.">
        </div>
        <p>The core idea of the TLU is to mimic a neuron's 'firing' based on inputs. Like you deciding to grab an umbrella if it's cloudy (input 1) or not (input 0), the TLU makes decisions based on simple on/off signals.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Threshold Logic Unit (TLU)</h4>
            <p>A mathematical model of a biological neuron that performs simple logic operations based on weighted inputs and a threshold.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(3)">Continue</div>
    </section>

    <section id="section3">
        <p>Fast forward to <strong>1957</strong>. <strong>Frank Rosenblatt</strong>, inspired by the TLU, created the <strong>Perceptron</strong>. This was a big leap! The Perceptron was the first artificial neural network that could actually <em>learn</em>. It had adjustable weights, which meant it could be trained to classify patterns. Imagine training a dog with treats â€“ the Perceptron learned by adjusting its weights based on feedback from its performance.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A photograph of Frank Rosenblatt with his Perceptron machine. The machine is large and has many wires and dials, giving a sense of the early hardware implementations of neural networks.">
        </div>
        <p>Here is Frank Rosenblatt himself presenting his invention: The Perceptron.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="An interactive diagram of a single-layer Perceptron. Users can toggle inputs (0 or 1), adjust weights (sliders), and see how the output changes. The weighted sum and activation (step function) are displayed, illustrating the Perceptron's decision process.">
        </div>
        <p>Try adjusting the weights and see how the output changes. This demonstrates how the Perceptron makes decisions based on weighted inputs.</p>
        <div class="continue-button" onclick="showNextSection(4)">Continue</div>
    </section>

    <section id="section4">
        <p>In the mid-1960s, researchers such as <strong>Alexey Ivakhnenko</strong> developed an early form of a feedforward multilayer network called <strong>Group Method of Data Handling (GMDH)</strong>. This was a big milestone, though it didn't immediately become mainstream. Progress slowed in the following years as hardware and algorithmic limitations became apparent. But then, in <strong>1970</strong>, a major breakthrough emerged: <strong>backpropagation</strong>. The foundational work by <strong>Seppo Linnainmaa</strong> on reverse-mode automatic differentiation laid the groundwork for this technique. Later, <strong>Paul Werbos</strong> (1974) applied these ideas to neural networks, and <strong>David Rumelhart</strong>, <strong>Geoffrey Hinton</strong>, and <strong>Ronald Williams</strong> popularized backpropagation in 1986. Think of it as a way to fine-tune the network by adjusting the weights based on how well it's performing.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Backpropagation</h4>
            <p>An algorithm for efficiently training neural networks by calculating the gradient of the loss function with respect to the network's weights.</p>
        </div>
        <div class="stop-and-think">
            <h3>Stop and Think</h3>
            <h4>Why do you think backpropagation was such a significant development? Discuss how it might have addressed the limitations of earlier models.</h4>
            <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
            <p id="stop-and-think-1" style="display: none;">Backpropagation was significant because it provided an efficient way to train multi-layer neural networks. Earlier models struggled with training deep networks due to the difficulty of adjusting weights in hidden layers. Backpropagation solved this by allowing the network to learn from its errors, propagating them backwards through the layers and adjusting weights accordingly. This enabled the creation of more complex and powerful neural networks capable of learning intricate patterns in data.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(5)">Continue</div>
    </section>

    <section id="section5">
        <p>The late 70s and early 80s saw the development of specialized networks. In <strong>1979</strong>, the precursors of <strong>Convolutional Neural Networks (CNNs)</strong> were developed, notably Kunihiko Fukushima's Neocognitron, which paved the way for modern CNNs. And in <strong>1982</strong>, <strong>John Hopfield</strong> introduced a form of recurrent neural network known as the Hopfield Network, illustrating how networks could store and retrieve patterns. These advances set the stage for further RNN designs that handle sequential data like text or audio. Each type of network was like a specialized tool, designed for a specific type of task.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A split diagram comparing a basic CNN and RNN. The CNN side shows layers processing an image, with feature maps highlighting edges and textures. The RNN side shows a sequence of inputs (e.g., words in a sentence) being processed over time, with connections looping back, illustrating the recurrent nature.">
        </div>
        <div class="continue-button" onclick="showNextSection(6)">Continue</div>
    </section>

    <section id="section6">
        <p>In <strong>1986</strong>, researchers successfully trained a multi-layered network using backpropagation. This was a huge moment! It demonstrated the practical power of this algorithm and paved the way for more complex and capable neural networks. Finally, networks could adjust their internal parameters to minimize the difference between predicted output and actual output across multiple layers, significantly improving the learning capability of neural networks.</p>
        <div class="why-it-matters">
            <h3>Why It Matters</h3>
            <p>This milestone proved that multi-layered neural networks could be effectively trained, opening the door for deeper and more powerful models.</p>
        </div>
        <div class="faq-section">
            <h3>Frequently Asked Questions</h3>
            <h4>Why is training multi-layered networks so important?</h4>
            <p>Multi-layered networks can learn hierarchical representations of data, allowing them to capture complex patterns that simpler models can't. This is crucial for tasks like image recognition and natural language processing.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(7)">Continue</div>
    </section>

    <section id="section7">
        <p>In <strong>1997</strong>, <strong>Long Short-Term Memory (LSTM)</strong> networks were introduced by Hochreiter and Schmidhuber. These were a special type of RNN that could remember information over longer periods. Think of it as giving the network a better memory. This was crucial for tasks like machine translation, where understanding the context of a sentence is essential. LSTMs addressed the vanishing gradient problem, a major hurdle in training deep networks. By introducing a memory cell and gating mechanisms, LSTMs could selectively remember or forget information over time, making them highly effective for tasks requiring long-range dependencies.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">LSTM (Long Short-Term Memory)</h4>
            <p>A type of recurrent neural network (RNN) that can learn long-range dependencies in sequential data.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(8)">Continue</div>
    </section>

    <section id="section8">
        <p>From <strong>2006 to 2012</strong>, we entered the era of <strong>deep learning</strong>. This period was marked by the development of techniques that allowed us to train very deep networks with many layers. Deeper networks could learn more complex patterns, leading to significant improvements in areas like image and speech recognition.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A diagram of a deep neural network with many layers. The layers are stacked vertically, with arrows indicating the flow of information. The network starts with an input layer, followed by multiple hidden layers, and ends with an output layer. The depth of the network is emphasized visually.">
        </div>
        <div class="continue-button" onclick="showNextSection(9)">Continue</div>
    </section>

    <section id="section9">
        <p>In <strong>2012</strong>, a deep learning model called <strong>AlexNet</strong> won the <strong>ImageNet Large Scale Visual Recognition Challenge</strong> by a wide margin. This was a watershed moment. It demonstrated the power of deep learning for computer vision tasks and ignited a revolution in the field. AlexNet's success was due to its deep architecture, use of GPUs for training, and techniques like ReLU activation and dropout, which helped prevent overfitting.</p>
        <div class="why-it-matters">
            <h3>Why It Matters</h3>
            <p>AlexNet's victory marked a turning point, proving that deep learning could achieve unprecedented accuracy on complex tasks, and sparking widespread interest and investment in the field.</p>
        </div>
        <div class="test-your-knowledge">
            <h3>Test Your Knowledge</h3>
            <h4>What was the significance of AlexNet winning the ImageNet competition in 2012?</h4>
            <button class="reveal-button" onclick="revealAnswer('test-your-knowledge-1')">Reveal Answer</button>
            <div id="test-your-knowledge-1" style="display: none;">
                <p><strong>Correct Answer:</strong> It demonstrated the power of deep learning for computer vision.</p>
                <p><strong>Explanation:</strong> AlexNet's large margin of victory showed that deep learning could achieve state-of-the-art results on a challenging image recognition task.</p>
            </div>
        </div>
        <div class="continue-button" onclick="showNextSection(10)">Continue</div>
    </section>

    <section id="section10">
        <p>And in <strong>2016</strong>, another milestone: <strong>AlphaGo</strong>, a deep learning system developed by Google, defeated the world champion in the game of Go. This was a huge achievement because Go is an incredibly complex game, and many thought it would be years before AI could master it.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A photograph of the AlphaGo match, showing a Go board and the human player contemplating his move. The image conveys the intensity of the competition and the significance of AI playing at a superhuman level.">
        </div>
        <div class="continue-button" onclick="showNextSection(11)">Continue</div>
    </section>

    <section id="section11">
        <p>From the simple TLU to AlphaGo's triumph, the journey of neural networks has been filled with twists and turns. We've seen periods of rapid progress and periods of stagnation. But through it all, the dream of creating intelligent machines has persisted. And today, thanks to the pioneers who laid the foundation, we're living in an era where neural networks are transforming our world in ways we never thought possible.</p>
    </section>

    <script>
        // Show the first section initially
        document.getElementById("section1").style.display = "block";
        document.getElementById("section1").style.opacity = "1";

        function showNextSection(nextSectionId) {
            const currentButton = event.target;
            const nextSection = document.getElementById("section" + nextSectionId);
            
            currentButton.style.display = "none";
            
            nextSection.style.display = "block";
            setTimeout(() => {
                nextSection.style.opacity = "1";
            }, 10);

            setTimeout(() => {
                nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 500);
        }

        function revealAnswer(id) {
            const revealText = document.getElementById(id);
            const revealButton = event.target;
            
            revealText.style.display = "block";
            revealButton.style.display = "none";
        }
    </script>
</body>
</html>
