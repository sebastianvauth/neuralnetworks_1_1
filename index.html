<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intro to Neural Networks - A Historical Journey</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            font-size: 150%;
        }
        section {
            margin-bottom: 20px;
            padding: 20px;
            background-color: #ffffff;
            display: none;
            opacity: 0;
            transition: opacity 0.5s ease-in;
        }
        h1, h2, h3, h4 {
            color: #333;
            margin-top: 20px;
        }
        p, li {
            line-height: 1.6;
            color: #444;
            margin-bottom: 20px;
        }
        ul {
            padding-left: 20px;
        }
        .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            text-align: left;
        }
        .image-placeholder img, .interactive-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .vocab-section {
            background-color: #f0f8ff;
        }
        .vocab-section h3 {
            color: #1e90ff;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .vocab-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .vocab-term {
            font-weight: bold;
            color: #1e90ff;
        }
        .why-it-matters {
            background-color: #ffe6f0;
        }
        .why-it-matters h3 {
            color: #d81b60;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .stop-and-think {
            background-color: #e6e6ff;
        }
        .stop-and-think h3 {
            color: #4b0082;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .continue-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #007bff;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .reveal-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #4b0082;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .test-your-knowledge {
            background-color: #e6ffe6;
        }
        .test-your-knowledge h3 {
            color: #28a745;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .test-your-knowledge h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .test-your-knowledge p {
            margin-bottom: 15px;
        }
        .check-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #28a745;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
            border: none;
            font-size: 1em;
        }
        .faq-section {
            background-color: #fffbea;
        }
        .faq-section h3 {
            color: #ffcc00;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .faq-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <section id="section1">
        <h1>Intro to Neural Networks - A Historical Journey</h1>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A retro futuristic cityscape with flying cars and old computers, symbolizing the historical yet forward-looking nature of neural network development.">
        </div>
        <h2>A Journey Through Time: The History of Neural Networks</h2>
        <p>Hello and welcome! Before we get our hands dirty building neural networks, let's take a fascinating trip back in time. Like any great invention, neural networks didn't just appear overnight. They have a rich and intriguing history, filled with brilliant minds, unexpected turns, and moments of both excitement and disappointment. Understanding this journey will not only give you context but also a deeper appreciation for the powerful technology we're about to explore.</p>
        <div class="continue-button" onclick="showNextSection(2)">Continue</div>
    </section>

    <section id="section2">
        <h2>The Spark of an Idea (1943)</h2>
        <p>Our story begins in <strong>1943</strong>, a time when computers were giant, room-filling machines, not the sleek devices in our pockets today. Imagine the world then...</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A stylized depiction of two scientists, McCulloch and Pitts, working together, with early neuron diagrams floating around them, representing their collaboration on the TLU model.">
        </div>
        <p>In this era, Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to propose a groundbreaking idea: <strong>the first mathematical model of an artificial neuron!</strong> They called it the <strong>"Threshold Logic Unit (TLU)"</strong>.</p>
        <p>Think of the TLU as the very first spark – the initial conceptualization of how we might mimic the brain's fundamental building block, the neuron, in machines. It was a rudimentary model, yes, but absolutely revolutionary for its time.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Threshold Logic Unit (TLU)</h4>
            <p>The first mathematical model of an artificial neuron, proposed in 1943 by McCulloch and Pitts. It laid the foundation for future neural network development.</p>
        </div>
        <p>This was the 'Big Bang' of neural networks, setting everything in motion!</p>
        <div class="stop-and-think">
            <h3>Stop and Think</h3>
            <h4>Why do you think the very first artificial neuron model was developed in the 1940s? What societal or technological factors might have inspired this?</h4>
            <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
            <p id="stop-and-think-1" style="display: none;">Consider the advancements in computing at the time, and the growing interest in understanding the human brain. Perhaps the war efforts and code-breaking also played a role in pushing computational thinking forward!</p>
        </div>
        <div class="continue-button" onclick="showNextSection(3)">Continue</div>
    </section>

    <section id="section3">
        <h2>The Perceptron Takes Center Stage (1957)</h2>
        <p>Fast forward to <strong>1957</strong>. Rock and roll is taking over the radio, and in the world of computing, a significant step forward is made by <strong>Frank Rosenblatt</strong>.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A portrait of Frank Rosenblatt next to an early diagram of a Perceptron, highlighting his contribution to neural network development.">
        </div>
        <p>Rosenblatt developed the <strong>Perceptron</strong>, the first practical and trainable artificial neuron! He envisioned it as the simplest, yet fundamental, building block for neural networks. Imagine it as a single, trainable unit capable of learning to classify patterns.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Animation showing a simplified Perceptron diagram. On click, the diagram highlights 'Inputs', 'Weights', 'Summation', 'Activation Function', and 'Output' sequentially with brief text descriptions appearing next to each part, explaining the basic function of a Perceptron.">
        </div>
        <p>See how it takes inputs, processes them with weights and an activation function to produce an output? That's the magic of the Perceptron in a nutshell!</p>
        <p>The Perceptron wasn't just a theoretical model; it could actually <strong>learn from data</strong>! This was a huge deal. It was trained using a learning algorithm that adjusted its internal parameters (weights) to improve its classification accuracy.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Perceptron</h4>
            <p>Developed by Frank Rosenblatt in 1957, the Perceptron is considered the simplest basic building block of neural networks. It is a trainable single-layer classifier.</p>
        </div>
        <p>Think of the Perceptron as the first 'student' in the neural network classroom, learning from examples!</p>
        <div class="faq-section">
            <h3>Frequently Asked Questions</h3>
            <h4>What kind of 'learning' did the Perceptron do?</h4>
            <p>The Perceptron learned through a process of adjusting its 'weights' based on errors it made during classification. If it misclassified an input, the weights were tweaked slightly to make it more likely to classify it correctly next time. This iterative process is the essence of machine learning!</p>
        </div>
        <div class="continue-button" onclick="showNextSection(4)">Continue</div>
    </section>

    <section id="section4">
        <h2>Multilayer Networks Emerge (1965)</h2>
        <p>The <strong>1960s</strong> were a time of exploration and expansion in many fields, and neural networks were no exception. By <strong>1965</strong>, researchers achieved another milestone: <strong>the first working feedforward multilayer network!</strong></p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A simplified diagram of a multilayer neural network with input, hidden, and output layers, symbolizing the advancement from single-layer perceptrons.">
        </div>
        <p>This was a crucial step. Single-layer networks like the Perceptron had limitations. They could only solve linearly separable problems. But by stacking layers of neurons together, researchers created networks capable of tackling much more complex tasks.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Diagram showing a simple animation illustrating the difference between a single-layer perceptron and a multilayer network. The single-layer perceptron struggles to classify non-linearly separable data, while the multilayer network successfully creates a more complex decision boundary.">
        </div>
        <p>See how adding layers allows for more intricate decision-making? This was a major leap in capability!</p>
        <p>This first multilayer network was successfully applied for <strong>supervised learning</strong>. Remember supervised learning? It's where we train models using labeled data – data where we already know the 'right answers'. This allowed these networks to learn from examples and make predictions on new, unseen data.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Feedforward Multilayer Network</h4>
            <p>A neural network architecture with multiple layers where data flows in one direction (forward) from the input layer to the output layer. This advancement allowed for solving more complex problems compared to single-layer networks.</p>
        </div>
        <p>This breakthrough opened up new possibilities for what neural networks could achieve!</p>
        <div class="test-your-knowledge">
            <h3>Test Your Knowledge</h3>
            <h4>What was the main limitation of single-layer perceptrons that multilayer networks aimed to overcome?</h4>
            <div id="quiz-1">
                <label><input type="radio" name="quiz-1" value="0"> They were too slow to train.</label><br>
                <label><input type="radio" name="quiz-1" value="1"> They could only solve linearly separable problems.</label><br>
                <label><input type="radio" name="quiz-1" value="2"> They required too much data.</label><br>
                <button class="check-button" onclick="checkAnswer('quiz-1', 1)">Check Answer</button>
            </div>
            <p id="quiz-1-feedback" style="display: none;"></p>
        </div>
        <div class="continue-button" onclick="showNextSection(5)">Continue</div>
    </section>

    <section id="section5">
        <h2>Backpropagation: The Training Revolution (1970)</h2>
        <p>The <strong>1970s</strong> brought another pivotal development: <strong>the backpropagation algorithm was developed around 1970!</strong> While the exact history is a bit complex with multiple researchers contributing, the algorithm's impact is undeniable.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A stylized representation of backpropagation, with arrows flowing backward through a neural network, indicating the flow of error signals during training.">
        </div>
        <p>Backpropagation was a game-changer. It provided an efficient way to train multilayer neural networks. Before backpropagation, training deep networks was incredibly difficult and computationally expensive.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Animation visually contrasts training a multilayer network *without* backpropagation (inefficient, slow progress) versus *with* backpropagation (efficient, rapid progress). Use simple graphs to represent training progress (e.g., loss decreasing over iterations).">
        </div>
        <p>See the dramatic difference in training efficiency? Backpropagation made deep networks truly trainable!</p>
        <p>This algorithm allowed networks to learn from their mistakes in a much more sophisticated way. It essentially calculates how much each weight in the network contributed to the error and then adjusts those weights to reduce the error. Think of it as a highly efficient feedback mechanism for learning.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Backpropagation Algorithm</h4>
            <p>A crucial algorithm developed around 1970 that enables the efficient training of multilayer neural networks by calculating gradients and updating weights to minimize errors.</p>
        </div>
        <p>Backpropagation was the key that unlocked the potential of deeper networks!</p>
        <div class="why-it-matters">
            <h3>Why It Matters</h3>
            <p>Backpropagation is arguably one of the most important algorithms in the history of neural networks. Without it, modern Deep Learning as we know it would not be possible. It's the engine that drives the learning process in most neural networks today.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(6)">Continue</div>
    </section>

    <section id="section6">
        <h2>Specialized Architectures Emerge (1979 & 1982)</h2>
        <p>The late <strong>1970s and early 1980s</strong> saw the development of specialized neural network architectures designed for specific types of data.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A collage of icons representing different data types: images, sequences of text, and time-series data, symbolizing the specialization of neural networks.">
        </div>
        <p>In <strong>1979</strong>, the <strong>precursors of Convolutional Neural Networks (CNNs)</strong> were developed. These architectures were inspired by the visual cortex and were designed to efficiently process spatial data, like images. Think of them as networks with a special 'eye' for patterns in images.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Convolutional Neural Networks (CNNs)</h4>
            <p>A type of neural network architecture developed starting in 1979, inspired by the visual cortex, and designed for processing spatial data like images. They are particularly effective in image recognition tasks.</p>
        </div>
        <p>CNNs were the first step towards building neural networks that could 'see' like humans!</p>
        <p>Then, in <strong>1982</strong>, <strong>Recurrent Neural Networks (RNNs)</strong> were developed. RNNs were designed to handle sequential data, like text, speech, or time series. They have a 'memory' component that allows them to process sequences of information over time. Imagine them as networks with a 'sense of time'!</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Recurrent Neural Networks (RNNs)</h4>
            <p>A type of neural network architecture developed in 1982, designed to process sequential data like text or time series. They incorporate feedback loops, allowing them to maintain a 'memory' of past inputs.</p>
        </div>
        <p>RNNs opened the door to processing data that unfolds over time, like language and videos!</p>
        <div class="stop-and-think">
            <h3>Stop and Think</h3>
            <h4>Why do you think specialized architectures like CNNs and RNNs were needed? Why couldn't a generic feedforward network handle all types of data effectively?</h4>
            <button class="reveal-button" onclick="revealAnswer('stop-and-think-2')">Reveal</button>
            <p id="stop-and-think-2" style="display: none;">Think about the inherent structure of different data types. Images have spatial relationships, text has sequential dependencies. Specialized architectures are designed to exploit these specific structures for better performance.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(7)">Continue</div>
    </section>

    <section id="section7">
        <h2>Training Deep Networks Becomes Reality (1986)</h2>
        <p>By <strong>1986</strong>, another significant step was achieved: <strong>the first multi-layered neural network was trained using backpropagation!</strong></p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A person successfully climbing a steep mountain representing the achievement of training deep multi-layered neural networks with backpropagation.">
        </div>
        <p>While backpropagation was developed earlier, it took some time to effectively apply it to train truly deep networks. This milestone showed that it was indeed possible to train networks with multiple hidden layers using backpropagation, paving the way for more complex models.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Animation shows a comparison: training a shallow network (quick, easy) vs. training a deep network *before* 1986 (difficult, slow) vs. training a deep network *after* 1986 with backpropagation (now feasible, faster). Use progress bars or similar visual cues.">
        </div>
        <p>Training deep networks was no longer just a dream – it was becoming a reality!</p>
        <p>This was a crucial confirmation that the theoretical power of backpropagation could be harnessed in practice to create and train more sophisticated neural networks.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Multi-layered Neural Network trained with Backpropagation</h4>
            <p>The successful training of a deep network using backpropagation in 1986 marked a practical realization of training deeper models, enabling more complex learning and representation.</p>
        </div>
        <p>This was a major step towards the Deep Learning era!</p>
        <div class="faq-section">
            <h3>Frequently Asked Questions</h3>
            <h4>Why was it difficult to train deep networks before 1986, even with backpropagation algorithm?</h4>
            <p>While backpropagation was known, challenges like the 'vanishing gradient problem' (gradients becoming too small in deep networks, hindering learning) and limitations in computing power made training deep networks effectively very difficult in practice until later improvements and hardware advancements.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(8)">Continue</div>
    </section>

    <section id="section8">
        <h2>Long Short-Term Memory (LSTM) RNNs (1997)</h2>
        <p>Despite some challenges, research continued, and in <strong>1997</strong>, a significant improvement to Recurrent Neural Networks emerged: <strong>Long Short-Term Memory (LSTM) networks were developed!</strong></p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A diagram comparing a simple RNN cell to a more complex LSTM cell, highlighting the increased complexity and memory capabilities of LSTMs.">
        </div>
        <p>LSTMs were designed to address a key limitation of standard RNNs: the vanishing gradient problem, especially when dealing with long sequences. LSTMs have a more complex internal structure that allows them to effectively learn long-range dependencies in sequential data.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Animation visually explains the 'vanishing gradient problem' in standard RNNs (gradients shrinking over long sequences) and contrasts it with how LSTMs' memory cells help mitigate this issue, allowing gradients to flow more effectively over longer sequences.">
        </div>
        <p>LSTMs provided RNNs with 'long-term memory', making them much more powerful for tasks like language processing!</p>
        <p>This breakthrough made RNNs much more practical for applications like natural language processing, speech recognition, and time series analysis, where long-term dependencies are crucial.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Long Short-Term Memory (LSTM) Networks</h4>
            <p>A specialized type of Recurrent Neural Network (RNN) developed in 1997 to address the vanishing gradient problem in standard RNNs. LSTMs are particularly effective in handling long-range dependencies in sequential data.</p>
        </div>
        <p>LSTMs were a crucial step in making RNNs truly useful for real-world sequence-based tasks!</p>
        <div class="test-your-knowledge">
            <h3>Test Your Knowledge</h3>
            <h4>What problem were LSTMs designed to solve in Recurrent Neural Networks?</h4>
            <div id="quiz-2">
                <label><input type="radio" name="quiz-2" value="0"> Overfitting to training data.</label><br>
                <label><input type="radio" name="quiz-2" value="1"> The vanishing gradient problem.</label><br>
                <label><input type="radio" name="quiz-2" value="2"> Slow training speed.</label><br>
                <button class="check-button" onclick="checkAnswer('quiz-2', 1)">Check Answer</button>
            </div>
            <p id="quiz-2-feedback" style="display: none;"></p>
        </div>
        <div class="continue-button" onclick="showNextSection(9)">Continue</div>
    </section>

    <section id="section9">
        <h2>The Deep Learning Era Begins (2006-2012)</h2>
        <p>Finally, in the period <strong>2006-2012</strong>, the stars aligned, and <strong>the era of Deep Learning began!</strong> This period marked a resurgence and explosion of neural network research and applications.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A 'sunrise' over a cityscape, symbolizing the dawn of the Deep Learning era, with neural network diagrams subtly incorporated into the city's skyline.">
        </div>
        <p>Several factors converged to trigger this Deep Learning revolution:</p>
        <ul>
            <li><strong>Increased Computing Power:</strong> GPUs (Graphics Processing Units), originally designed for gaming, became powerful and affordable enough to train large neural networks.</li>
            <li><strong>Larger Datasets:</strong> The internet and digital data explosion provided massive amounts of labeled data (like ImageNet) needed to train complex models.</li>
            <li><strong>Algorithmic Improvements:</strong> Continued research led to better activation functions (like ReLU), initialization techniques, and network architectures.</li>
        </ul>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Deep Learning Era</h4>
            <p>The period starting around 2006-2012 marked by a resurgence and rapid advancement of neural networks, driven by increased computing power, larger datasets, and algorithmic improvements, leading to breakthroughs in various fields.</p>
        </div>
        <p>These factors combined created the perfect storm for Deep Learning to take off!</p>
        <div class="why-it-matters">
            <h3>Why It Matters</h3>
            <p>The Deep Learning era is not just a historical period; it's the era we are living in <em>now</em>. It's the reason why AI is suddenly so powerful and prevalent in our lives. It's the foundation for self-driving cars, advanced language translation, image recognition, and much more. Understanding this historical context helps us appreciate the magnitude of this technological shift.</p>
        </div>
        <div class="stop-and-think">
            <h3>Stop and Think</h3>
            <h4>Which of the factors (computing power, data, algorithms) do you think was <em>most</em> crucial for the Deep Learning revolution? Why?</h4>
            <button class="reveal-button" onclick="revealAnswer('stop-and-think-3')">Reveal</button>
            <p id="stop-and-think-3" style="display: none;">It's hard to pick just one! They all played interconnected roles. Perhaps the availability of massive datasets unlocked by the internet was a key catalyst, as powerful algorithms and hardware could then truly shine.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(10)">Continue</div>
    </section>

    <section id="section10">
        <h2>AlexNet Wins ImageNet (2012)</h2>
        <p>In <strong>2012</strong>, a defining moment arrived: <strong>AlexNet, a deep learning network, won the ImageNet Competition by a wide margin!</strong></p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A stylized 'AlexNet' architecture diagram superimposed over the ImageNet logo, emphasizing AlexNet's groundbreaking performance in the competition.">
        </div>
        <p>AlexNet wasn't just a little better; it dramatically outperformed all previous approaches in image recognition. This victory was a resounding demonstration of the power of Deep Learning, capturing the attention of the research community and the world.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Graph visually compares AlexNet's error rate in the ImageNet competition to previous years' best performances. Emphasize the sharp drop in error rate.">
        </div>
        <p>This wasn't just progress; it was a paradigm shift! AlexNet showed the world what Deep Learning could really do.</p>
        <p>This event is often considered a major turning point, solidifying Deep Learning's dominance in image recognition and beyond. It sparked a surge of interest and investment in the field, accelerating its progress even further.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">AlexNet</h4>
            <p>A groundbreaking deep convolutional neural network that won the 2012 ImageNet Competition by a significant margin. Its success marked a turning point for Deep Learning, demonstrating its power for image recognition.</p>
        </div>
        <p>AlexNet's victory was the 'proof of concept' that ignited the current Deep Learning boom!</p>
        <div class="test-your-knowledge">
            <h3>Test Your Knowledge</h3>
            <h4>What was the significance of AlexNet's win in the 2012 ImageNet Competition?</h4>
            <div id="quiz-3">
                <label><input type="radio" name="quiz-3" value="0"> It proved that neural networks could be used for image recognition.</label><br>
                <label><input type="radio" name="quiz-3" value="1"> It demonstrated the superior performance of Deep Learning for image recognition, outperforming previous approaches significantly.</label><br>
                <label><input type="radio" name="quiz-3" value="2"> It was the first time a neural network was used in a competition.</label><br>
                <button class="check-button" onclick="checkAnswer('quiz-3', 1)">Check Answer</button>
            </div>
            <p id="quiz-3-feedback" style="display: none;"></p>
        </div>
        <div class="continue-button" onclick="showNextSection(11)">Continue</div>
    </section>

    <section id="section11">
        <h2>AlphaGo Beats the Go World Champion (2016)</h2>
        <p>In <strong>2016</strong>, another landmark achievement stunned the world: <strong>AlphaGo, a program from Google DeepMind, beat the Go world champion!</strong></p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A Go board with game pieces arranged in a complex pattern, with a subtle 'AlphaGo' logo overlay, representing the AI's victory in the game of Go.">
        </div>
        <p>Go is an incredibly complex board game, considered far more challenging for AI than chess. AlphaGo's victory demonstrated Deep Learning's ability to tackle strategic reasoning and complex decision-making at a superhuman level.</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Video snippet (or animated GIF) showing a highlight from a game of AlphaGo vs. Lee Sedol, emphasizing the strategic depth and surprising moves made by AlphaGo.">
        </div>
        <p>This was more than just a game; it was a symbol of AI's rapid progress in mastering complex human-level tasks!</p>
        <p>AlphaGo combined Deep Learning with Reinforcement Learning, showing the power of integrating different AI techniques. It was a testament to the versatility and potential of Deep Learning in tackling highly complex problems beyond pattern recognition.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">AlphaGo</h4>
            <p>A Deep Learning program from Google DeepMind that defeated the Go world champion in 2016. It showcased Deep Learning's capabilities in strategic reasoning and complex decision-making.</p>
        </div>
        <p>AlphaGo proved that Deep Learning could conquer even the most challenging 'games' – and by extension, complex real-world problems!</p>
        <div class="why-it-matters">
            <h3>Why It Matters</h3>
            <p>AlphaGo's victory was a powerful demonstration of Deep Learning's potential to solve problems that require strategic thinking and creativity, domains previously thought to be exclusively human. It further fueled the excitement and investment in AI, pushing the field forward at an unprecedented pace.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(12)">Continue</div>
    </section>

    <section id="section12">
        <h2>Review and Reflect</h2>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A winding road leading into a bright future cityscape, symbolizing the journey through the history of neural networks and the exciting future ahead.">
        </div>
        <p>Wow, what a journey! From the initial spark of the TLU to the groundbreaking achievements of AlphaGo, we've seen how neural networks have evolved over decades. In this lesson, we explored:</p>
        <ul>
            <li><strong>Key Milestones:</strong> From the first artificial neuron to Deep Learning's modern triumphs.</li>
            <li><strong>Key Figures:</strong> Like McCulloch, Pitts, and Rosenblatt, who laid the foundations.</li>
            <li><strong>The Evolution:</strong> From simple Perceptrons to complex architectures like CNNs, RNNs, and LSTMs.</li>
            <li><strong>The Deep Learning Revolution:</strong> Driven by data, computing power, and algorithmic breakthroughs.</li>
        </ul>
        <p>Understanding this history is not just about dates; it's about appreciating the long and persistent effort that has brought us to where we are today. And now, you are ready to start building your own neural networks, standing on the shoulders of giants!</p>
    </section>

    <script>
        // Show the first section initially
        document.getElementById("section1").style.display = "block";
        document.getElementById("section1").style.opacity = "1";

        function showNextSection(nextSectionId) {
            const currentButton = event.target;
            const nextSection = document.getElementById("section" + nextSectionId);
            
            currentButton.style.display = "none";
            
            nextSection.style.display = "block";
            setTimeout(() => {
                nextSection.style.opacity = "1";
            }, 10);

            setTimeout(() => {
                nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 500);
        }

        function revealAnswer(id) {
            const revealText = document.getElementById(id);
            const revealButton = event.target;
            
            revealText.style.display = "block";
            revealButton.style.display = "none";
        }

        function checkAnswer(quizId, correctAnswer) {
            const selectedAnswer = document.querySelector(`input[name="${quizId}"]:checked`);
            const feedback = document.getElementById(`${quizId}-feedback`);
            
            if (selectedAnswer) {
                if (parseInt(selectedAnswer.value) === correctAnswer) {
                    feedback.textContent = "Correct! Well done!";
                    feedback.style.color = "green";
                } else {
                    feedback.textContent = "Not quite. Try again!";
                    feedback.style.color = "red";
                }
                feedback.style.display = "block";
            } else {
                feedback.textContent = "Please select an answer.";
                feedback.style.color = "blue";
                feedback.style.display = "block";
            }
        }
    </script>
</body>
</html>
